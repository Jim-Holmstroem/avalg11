\subsection{Benchmark}
To benchmark the algorithm we measured the execution time for 100 randomly sampled 
semiprimes $p*q=n$ with a bitlength of $p,q=(u,v)$
and the factorization is symmetric on $p,q$ so we only did measurements on $u \le v$.
The measure-points  $(u,v) \in 4[1,12]^2$ where used.

%the upper triangular matrix and then apply the results transposed.

By this measure we are getting the characteristics in the base problem of 
factoring, factoring out two primes from each other.

We also did some comparisons between some algorithms and their parameters by 
directly getting points from \texttt{KATTIS}~\cite{kattis} on the problem 
\texttt{oldkattis:factoring}~\cite{factoring}.

\subsection{Test system}
All measurements was made on a computer using Ubuntu 10.04 LTS (64-bit)
on a Intel Xeon X3470 CPU @2.93GHz with 4 cores (8 threads) and with 16GB RAM. 
The code was compiled with \texttt{gcc-4.4.3} using the flag \texttt{-g} and the library \texttt{GMP-4.3.2}.

The execution time was measured with the \texttt{USER} time from the 
\texttt{time} command.

\subsection{Test results}

\subsubsection{Benchmarks}
%%To benchmark the algorithm we used a matrix $M_{u,v}$ of 100 randomly sampled semiprimes $p*q=n$ with the bitlength of $p,q=(u,v)$ 
%%and because the factorization is symmetric on $p,q$
%%<footnote?>(since multiplication is commutative, p \leftrightarrow q we still get the same resulting $n$)</footnote>
%%we only do measurements on the upper(/lower) triangular matrix and then apply the results transposed.
%%<footnote?>\verbatim+M = M+(M'-diag(diag(M)))+</footnote>
%%By this measure we are getting the characteristics in the base problem of factoring, factoring out two primes from each other, finding the needle in the haystack, the only two numbers that divides $n$.
%%We clearly see an exponential trend on the number of bits.


Note that the measure-point $(48,48)$ was never measured is was extrapolated 
and we added linear interpolation between the measurements to get a more visible mesh.

%!!!<pollard_plot.eps>


%%We clearly see an exponential trend on the number of bits.
To add clarity we also transformed the time scale with $t'=log(t+0.1)$, the $+0.1$ is to avoid zeros.

%!!!<pollard_plot_log.eps>

%For added clarity we transformed the time scale with $log(0.1+t)$. 
%The $+0.1$ was to avoid the zeroes in our measurements, 
%basically we say that 0.0 is 0.1 and so forth, this is 
%not effecting the measurements noticeably

%to instead of time getting the order of time.


%As one can see it clearly reassembles the shape of \verbatim+min+$(u,v)$ it only differs some and that are 
%probably due to the aritmetic on larger number even do the smallest number as small as the other ones,
%but the \verbatim+min$(u,v)$-shape still dominates. Giving us an hint on the running time $O(2^min(u,v))$.

%As one can see it clearly reassembles the shape of \verbatim+min+$(u,v)$ it 
%differ some and that are 
%probably due to the arithmetic on larger number even do the smallest number 
%as small as the other ones,
%but the \verbatim+min+$(u,v)$-shape still dominates.

%<trial_divide_semilogx.eps>

\subsubsection{Kattis}
These tests were solely based on the score and running time on  the 
\texttt{KATTIS} problem \texttt{oldkattis:factoring}.

\paragraph{Primality testing}
Using only the Miller-Rabin tests do determine if a number is prime and
if so print only this number. The results can be seen in table
\ref{table:miller-rabin}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Score} & \textbf{Running time} \\ \hline
    1              & 0.01                  \\ \hline
    \end{tabular}
    \caption{Only using the Miller-Rabin test}
    \label{table:miller-rabin}
\end{table}

\paragraph{Trial division and primality test}
The next version uses trial division (as described in algorithm
\ref{alg:naive}) and the Miller-Rabin primality test. A fixed number of primes were
used for primality testing, but the number of primes we used varied. 
The results can be seen in table \ref{table:trial}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Number of primes} & \textbf{Score} & \textbf{Running time} \\ \hline
    10000 & 21 & 0.05 \\ \hline
    7500  & 21 & 0.05 \\ \hline
    5000  & 21 & 0.04 \\ \hline
    2500  & 21 & 0.02 \\ \hline
    1875  & 21 & 0.02 \\ \hline
    1718  & 21 & 0.02 \\ \hline 
    1562  & 20 & 0.02 \\ \hline 
    1250  & 19 & 0.02 \\ \hline
    625   & 18 & 0.01 \\ \hline
    312   & 18 & 0.01 \\ \hline
    156   & 15 & 0.01 \\ \hline
    78    & 14 & 0.01 \\ \hline
    39    & 13 & 0.01 \\ \hline
    19    & 9  & 0.01 \\ \hline
    10    & 7  & 0.01 \\ \hline
    5     & 4  & 0.01 \\ \hline
    0     & 1  & 0.01 \\ \hline
    \end{tabular}
    \caption{Using trial division and primality test}
    \label{table:trial}
\end{table}


\paragraph{Pollard's $\rho$}
Pollard's $\rho$ algorithm was tested in a few different ways. The different
combinations used were:
\begin{enumerate}[A)]
    \item Only Pollard's $\rho$ with Floyd's cycle detection and primality test
    \item Only Pollard's $\rho$ with Brent's cycle detection and primality test
    \item Pollard's $\rho$ with Brent's cycle detection, trial division and 
          primality test
    \item Pollard's $\rho$ with Floyd's cycle detection, handling perfect
          powers, trial division and primality test
\end{enumerate}
The results can be seen in table \ref{table:pollard-comparison}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Variant} & \textbf{Score} \\ \hline
    A                & 72             \\ \hline
    B                & 64             \\ \hline
    C                & 71             \\ \hline
    D                & 72             \\ \hline
    \end{tabular}
    \caption{Comparing different implementations Pollard's $\rho$ algorithm}
    \label{table:pollard-comparison}
\end{table}

Given that Floyd's cycle detection seemed superior to Brent's cycle detection,
we, Pollard's $\rho$ algorithm with Floyd's cycle detection was tested with 
two different random value generators. 
The first one used was GMPs \texttt{mp\_urandomm} and the second
being used was \texttt{rand}. The results can be seen in table
\ref{table:random}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Variant}     & \textbf{Score} \\ \hline
    \texttt{mp\_urandomm} & 72             \\ \hline
    \texttt{rand}        & 75             \\ \hline
    \end{tabular}
    \caption{Comparing different random generators}
    \label{table:random}
\end{table}

\paragraph{Algorithm cut-off}
As a final test, Pollard's $\rho$ algorithm was tested with different cut-off
values and the result can be seen in table \ref{table:cutoff} (see section for
\ref{sec:time-outs} more details about the cut-off value).

\emph{Note:} this algorithm used \texttt{mp\_urandomm}, therefore the maximum
score was 72.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Cut-off value} & \textbf{Score} & \textbf{Running time} \\ \hline
    190000 & 72 & 14.32 \\ \hline
    172500 & 71 & 13.05 \\ \hline
    155000 & 71 & 11.93 \\ \hline
    137500 & 71 & 10.97 \\ \hline
    120000 & 70 & 10.01 \\ \hline
    110000 & 70 & 9.50 \\ \hline
    100000 & 69 & 8.70 \\ \hline
    90000  & 66 & 8.21 \\ \hline
    80000  & 65 & 7.48 \\ \hline 
    70000  & 65 & 6.77 \\ \hline   
    60000  & 63 & 6.00 \\ \hline
    50000  & 58 & 5.22 \\ \hline
    40000  & 56 & 4.28 \\ \hline
    35000  & 55 & 3.83 \\ \hline
    30000  & 50 & 3.35 \\ \hline
    25000  & 49 & 2.86 \\ \hline
    20000  & 44 & 2.37 \\ \hline
    15000  & 41 & 1.82 \\ \hline
    10000  & 38 & 1.25 \\ \hline
    5000   & 35 & 0.67 \\ \hline
    0      & 1  & 0.01 \\ \hline
    \end{tabular}
    \caption{Comparing different cut-off values}
    \label{table:cutoff}
\end{table}

\subsection{Kattis submission}
The best Kattis submission has the id 256166.
